<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Docstrings · Glowe.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Glowe.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Readme</a></li><li class="current"><a class="toctext" href>Docstrings</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Docstrings</a></li></ul></nav><hr/><div id="topbar"><span>Docstrings</span><a class="fa fa-bars" href="#"></a></div></header><p>Package doesn&#39;t contain Documenter docs.</p><p>Docs automatically generated by juliadocs.org</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.size-Tuple{WordVectors}" href="#Base.size-Tuple{WordVectors}"><code>Base.size</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">size(wv)</code></pre><p>Return the word vector length and the number of words as a tuple.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.analogy-Union{Tuple{H}, Tuple{T}, Tuple{S}, Tuple{WordVectors{S,T,H},AbstractArray,AbstractArray}, Tuple{WordVectors{S,T,H},AbstractArray,AbstractArray,Any}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString" href="#Glowe.analogy-Union{Tuple{H}, Tuple{T}, Tuple{S}, Tuple{WordVectors{S,T,H},AbstractArray,AbstractArray}, Tuple{WordVectors{S,T,H},AbstractArray,AbstractArray,Any}} where H&lt;:Integer where T&lt;:Real where S&lt;:AbstractString"><code>Glowe.analogy</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">analogy(wv, pos, neg, n=5)</code></pre><p>Compute the analogy similarity between two lists of words. The positions and the similarity values of the top <code>n</code> similar words will be returned. For example, <code>king - man + woman = queen</code> will be <code>pos=[&quot;king&quot;, &quot;woman&quot;], neg=[&quot;man&quot;]</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.analogy_words" href="#Glowe.analogy_words"><code>Glowe.analogy_words</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">analogy_words(wv, pos, neg, n=5)</code></pre><p>Return the top <code>n</code> words computed by analogy similarity between positive words <code>pos</code> and negaive words <code>neg</code>. from the WordVectors <code>wv</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.cooccur-Tuple{AbstractString,AbstractString,AbstractString}" href="#Glowe.cooccur-Tuple{AbstractString,AbstractString,AbstractString}"><code>Glowe.cooccur</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">cooccur(corpus, vocab, cooccurrences; verbose=2, symmetric=0, window_size=15, memory=4.0, max_product=nothing, overflow_length=nothing, overflow_file=&quot;overflow&quot;, distance_weighting=1)</code></pre><p>Calculates word-word cooccurrence statistics.</p><p><strong>Arguments</strong></p><ul><li><code>corpus::AbstractString</code> input corpus file path</li><li><code>vocab::AbstractString</code> input vocabulary file path (the vocabulary contains truncated unigram counts, produced by <code>vocab_count</code>)</li><li><code>cooccurrences::AbstractString</code> output cooccurrences file path</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose::Int</code> set verbosity: 0, 1, 2 (default) or 3</li><li><code>symmetric::Int</code> if &lt;int&gt; = 0, only use left context; if &lt;int&gt; = 1 (default), use left and right</li><li><code>window_size::Int</code> number of context words to the left (and to the right, if symmetric = 1); default 15</li><li><code>memory::Float64</code> soft limit for memory consumption, in GB – based on simple heuristic, so not extremely accurate; default 4.0</li><li><code>max_product::Union{Nothing, Int}</code> limit the size of dense cooccurrence array by specifying the max product &lt;int&gt; of the frequency counts of the two cooccurring words. This value overrides that which is automatically produced by <code>memory</code>. Typically only needs adjustment for use with very large corpora</li><li><code>overflow_length::Union{Nothing, Int}</code> limit to length &lt;int&gt; the sparse overflow array, which buffers cooccurrence data that does not fit in the dense array, before writing to disk. This value overrides that which is automatically produced by <code>memory</code>. Typically only needs adjustment for use with very large corpora</li><li><code>overflow_file::String</code> filename, excluding extension, for temporary files; default &quot;overflow&quot;</li><li><code>distance_weighting::Int</code> if &lt;int&gt; = 0, do not weight cooccurrence count by distance between words; if &lt;int&gt; = 1 (default), weight the cooccurrence count by inverse of distance between words&quot;</li></ul><p><strong>Examples</strong></p><pre><code class="language-none"># It is assumed that vocab.txt exists and has been created by `vocab_count`
julia&gt; cooccur(&quot;corpus.txt&quot;, &quot;vocab.txt&quot;, &quot;cooccurrences.bin&quot;, verbose=2, symmetric=0, window_size=10, memory=8.0, overflow_file=&quot;tempoverflow&quot;)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.cosine" href="#Glowe.cosine"><code>Glowe.cosine</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">cosine(wv, word, n=10)</code></pre><p>Return the position of <code>n</code> (by default <code>n = 10</code>) neighbors of <code>word</code> and their cosine similarities.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.cosine_similar_words" href="#Glowe.cosine_similar_words"><code>Glowe.cosine_similar_words</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">cosine_similar_words(wv, word, n=10)</code></pre><p>Return the top <code>n</code> (by default <code>n = 10</code>) most similar words to <code>word</code> from the WordVectors <code>wv</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.get_vector-Tuple{WordVectors,Any}" href="#Glowe.get_vector-Tuple{WordVectors,Any}"><code>Glowe.get_vector</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">get_vector(wv, word [; oov=false, oov_key=&quot;&lt;unk&gt;&quot;)</code></pre><p>Returns the vector representation of <code>word</code> from the WordVectors <code>wv</code>. If <code>oov</code> is <code>true</code>, the vector corresponding to the key <code>oov_key</code> is returned for out-of-vocabulary words.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.in_vocabulary-Tuple{WordVectors,AbstractString}" href="#Glowe.in_vocabulary-Tuple{WordVectors,AbstractString}"><code>Glowe.in_vocabulary</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">in_vocabulary(wv, word)</code></pre><p>Return <code>true</code> if <code>word</code> is part of the vocabulary of the WordVector <code>wv</code> and <code>false</code> otherwise.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.index-Tuple{WordVectors,Any}" href="#Glowe.index-Tuple{WordVectors,Any}"><code>Glowe.index</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">index(wv, word)</code></pre><p>Return the index of <code>word</code> from the WordVectors <code>wv</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.shuffle-Tuple{AbstractString,AbstractString}" href="#Glowe.shuffle-Tuple{AbstractString,AbstractString}"><code>Glowe.shuffle</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">shuffle(cooccurences, shuffled; verbose=2, memory=4.0, array_size=nothing, temp_file=&quot;temp_shuffle&quot;)</code></pre><p>Shuffles entries of word-word cooccurrence files.</p><p><strong>Arguments</strong></p><ul><li><code>cooccurences::AbstractString</code> input cooccurrences file path</li><li><code>shuffled::AbstractString</code> output shuffled cooccurences file path</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose::Int</code> set verbosity: 0, 1, or 2 (default)</li><li><code>memory::Float64</code> soft limit for memory consumption, in GB; default 4.0</li><li><code>array_size::Union{Nothing, Int}</code> limit to length &lt;int&gt; the buffer which stores chunks of data to shuffle before writing to disk. This value overrides that which is automatically produced by <code>memory</code></li><li><code>temp_file</code>::String filename, excluding extension, for temporary files; default &quot;temp_shuffle&quot;</li></ul><p><strong>Examples</strong></p><pre><code class="language-none"># It is assumed that cooccurences.bin exists and has been created by `cooccur`
julia&gt; shuffle(&quot;cooccurences.bin&quot;, &quot;shuffled.bin&quot;, verbose=2, memory=8.0)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.similarity-Tuple{WordVectors,Any,Any}" href="#Glowe.similarity-Tuple{WordVectors,Any,Any}"><code>Glowe.similarity</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">similarity(wv, word1, word2)</code></pre><p>Return the cosine similarity value between two words <code>word1</code> and <code>word2</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.vocab_count-Tuple{AbstractString,AbstractString}" href="#Glowe.vocab_count-Tuple{AbstractString,AbstractString}"><code>Glowe.vocab_count</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">vocab_count(corpus, vocab; max_vocab=100_000, min_count=10, verbose=2)</code></pre><p>Extracts unigram counts.</p><p><strong>Arguments</strong></p><ul><li><code>corpus::AbstractString</code> input corpus file path</li><li><code>vocab::AbstractString</code> output vocabulary file path</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose::Int</code> set verbosity: 0, 1 or 2 (default)</li><li><code>max_vocab::Int</code> upper bound on vocabulary size, i.e. keep the &lt;int&gt; most frequent words. The minimum frequency words are randomly sampled so as to obtain an even distribution over the alphabet.</li><li><code>min_count::Int</code> lower limit such that words which occur fewer than &lt;int&gt; times are discarded.</li></ul><p><strong>Examples</strong></p><pre><code class="language-none">julia&gt; vocab_count(&quot;corpus.txt&quot;, &quot;vocab.txt&quot;, verbose=2, max_vocab=100_000, min_count=10)</code></pre></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.vocabulary-Tuple{WordVectors}" href="#Glowe.vocabulary-Tuple{WordVectors}"><code>Glowe.vocabulary</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">vocabulary(wv)</code></pre><p>Return the vocabulary as a vector of words of the WordVectors <code>wv</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.wordvectors-Union{Tuple{T}, Tuple{AbstractString,Type{T}}} where T&lt;:Real" href="#Glowe.wordvectors-Union{Tuple{T}, Tuple{AbstractString,Type{T}}} where T&lt;:Real"><code>Glowe.wordvectors</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">wordvectors(filename [,type=Float64][; kind=:text, header=false, normalize=true, vocabulary=nothing])</code></pre><p>Generate a WordVectors type object from a file.</p><p><strong>Arguments</strong></p><ul><li><code>filename::AbstractString</code> the embeddings file name</li><li><code>type::Type</code> type of the embedding vector elements; default <code>Float64</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>kind::Symbol</code> specifies whether the embeddings file is textual (<code>:text</code>)</li></ul><p>or binary (<code>:binary</code>); default <code>:text</code></p><ul><li><code>header::Union{Nothing, Bool}</code> in text embeddings files specifies</li></ul><p>whether the file contains a header i.e. number of lines, columns or not.   If the header is <code>nothing</code>, the loader will attempt to autodetect the   presence of a header; default <code>nothing</code></p><ul><li><code>normalize:Bool</code> specifies whether to normalize the embedding vectors</li></ul><p>i.e. return unit vectors; default true</p><ul><li><code>vocabulary::Union{Nothing, AbstractString}</code> path to the vocabulary</li></ul><p>file generated with <code>vocab_count</code> (needed for binary embeddings); default <code>nothing</code></p><ul><li><code>load_bias::Bool</code> specifies whether to load the bias term or not</li></ul><p>if using binary embedding files; default <code>false</code></p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Glowe.autodetect_header-Tuple{AbstractString}" href="#Glowe.autodetect_header-Tuple{AbstractString}"><code>Glowe.autodetect_header</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">autodetect_header(filename)</code></pre><p>Function that attempts at autodetecting the presence of a header in a GloVe embeddings file in a text format. If the function fails to detect a header, a <code>false</code> value is returned.</p><p>Note: The function explicitly expects a text format for the embeddings. The behaviour is undetermined for binary formats and no attempt at detecting the file format is done.</p></div></div></section><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Readme</span></a></footer></article></body></html>
