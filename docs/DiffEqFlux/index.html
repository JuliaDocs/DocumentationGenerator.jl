<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Readme · DiffEqFlux.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>DiffEqFlux.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Readme</a><ul class="internal"><li><a class="toctext" href="#Example-Usage-1">Example Usage</a></li><li><a class="toctext" href="#Use-with-GPUs-1">Use with GPUs</a></li><li><a class="toctext" href="#API-Documentation-1">API Documentation</a></li></ul></li><li><a class="toctext" href="autodocs/">Docstrings</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Readme</a></li></ul></nav><hr/><div id="topbar"><span>Readme</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="DiffEqFlux.jl-1" href="#DiffEqFlux.jl-1">DiffEqFlux.jl</a></h1><p><a href="https://gitter.im/JuliaDiffEq/Lobby?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge"><img src="https://badges.gitter.im/JuliaDiffEq/Lobby.svg" alt="Join the chat at https://gitter.im/JuliaDiffEq/Lobby"/></a> <a href="https://travis-ci.org/JuliaDiffEq/DiffEqFlux.jl"><img src="https://travis-ci.org/JuliaDiffEq/DiffEqFlux.jl.svg?branch=master" alt="Build Status"/></a> <a href="https://ci.appveyor.com/project/ChrisRackauckas/diffeqflux-jl"><img src="https://ci.appveyor.com/api/projects/status/e5a9pad58ojo26ir?svg=true" alt="Build status"/></a></p><p>DiffEqFlux.jl fuses the world of differential equations with machine learning by helping users put diffeq solvers into neural networks. This package utilizes <a href="http://docs.juliadiffeq.org/latest/">DifferentialEquations.jl</a> and <a href="https://fluxml.ai/">Flux.jl</a> as its building blocks.</p><h2><a class="nav-anchor" id="Example-Usage-1" href="#Example-Usage-1">Example Usage</a></h2><p>For an overview of what this package is for, <a href=".">see this blog post</a>.</p><h3><a class="nav-anchor" id="Optimizing-parameters-of-an-ODE-1" href="#Optimizing-parameters-of-an-ODE-1">Optimizing parameters of an ODE</a></h3><p>First let&#39;s create a Lotka-Volterra ODE using DifferentialEquations.jl. For more details, <a href=".">see the DifferentialEquations.jl documentation</a></p><pre><code class="language-julia">using DifferentialEquations
function lotka_volterra(du,u,p,t)
  x, y = u
  α, β, δ, γ = p
  du[1] = dx = α*x - β*x*y
  du[2] = dy = -δ*y + γ*x*y
end
u0 = [1.0,1.0]
tspan = (0.0,10.0)
p = [1.5,1.0,3.0,1.0]
prob = ODEProblem(lotka_volterra,u0,tspan,p)
sol = solve(prob,Tsit5())
using Plots
plot(sol)</code></pre><p><img src="https://user-images.githubusercontent.com/1814174/51388169-9a07f300-1af6-11e9-8c6c-83c41e81d11c.png" alt="LV Solution Plot"/></p><p>Next we define a single layer neural network that uses the <code>diffeq_rd</code> layer function that takes the parameters and returns the solution of the <code>x(t)</code> variable. Instead of being a function of the parameters, we will wrap our parameters in <code>param</code> to be tracked by Flux:</p><pre><code class="language-julia">using Flux, DiffEqFlux
p = param([2.2, 1.0, 2.0, 0.4]) # Initial Parameter Vector
params = Flux.Params([p])

function predict_rd() # Our 1-layer neural network
  Array(diffeq_rd(p,prob,Tsit5(),saveat=0.1))
end</code></pre><p>Next we choose a loss function. Our goal will be to find parameter that make the Lotka-Volterra solution constant <code>x(t)=1</code>, so we defined our loss as the squared distance from 1:</p><pre><code class="language-julia">loss_rd() = sum(abs2,x-1 for x in predict_rd())</code></pre><p>Lastly, we train the neural network using Flux to arrive at parameters which optimize for our goal:</p><pre><code class="language-julia">data = Iterators.repeated((), 100)
opt = ADAM(0.1)
cb = function () #callback function to observe training
  display(loss_rd())
  # using `remake` to re-create our `prob` with current parameters `p`
  display(plot(solve(remake(prob,p=Flux.data(p)),Tsit5(),saveat=0.1),ylim=(0,6)))
end

# Display the ODE with the initial parameter values.
cb()

Flux.train!(loss_rd, params, data, opt, cb = cb)</code></pre><p><img src="https://user-images.githubusercontent.com/1814174/51399500-1f4dd080-1b14-11e9-8c9d-144f93b6eac2.gif" alt="Flux ODE Training Animation"/></p><p>Note that by using anonymous functions, this <code>diffeq_rd</code> can be used as a layer in a neural network <code>Chain</code>, for example like</p><pre><code class="language-julia">m = Chain(
  Conv((2,2), 1=&gt;16, relu),
  x -&gt; maxpool(x, (2,2)),
  Conv((2,2), 16=&gt;8, relu),
  x -&gt; maxpool(x, (2,2)),
  x -&gt; reshape(x, :, size(x, 4)),
  # takes in the ODE parameters from the previous layer
  p -&gt; Array(diffeq_rd(p,prob,Tsit5(),saveat=0.1),
  Dense(288, 10), softmax) |&gt; gpu</code></pre><p>or</p><pre><code class="language-julia">m = Chain(
  Dense(28^2, 32, relu),
  # takes in the initial condition from the previous layer
  x -&gt; Array(diffeq_rd(p,prob,Tsit5(),saveat=0.1,u0=x))),
  Dense(32, 10),
  softmax)</code></pre><h3><a class="nav-anchor" id="Using-Other-Differential-Equations-1" href="#Using-Other-Differential-Equations-1">Using Other Differential Equations</a></h3><p>Other differential equation problem types from DifferentialEquations.jl are supported. For example, we can build a layer with a delay differential equation like:</p><pre><code class="language-julia">function delay_lotka_volterra(du,u,h,p,t)
  x, y = u
  α, β, δ, γ = p
  du[1] = dx = (α - β*y)*h(p,t-0.1)[1]
  du[2] = dy = (δ*x - γ)*y
end
h(p,t) = ones(eltype(p),2)
prob = DDEProblem(delay_lotka_volterra,[1.0,1.0],h,(0.0,10.0),constant_lags=[0.1])

p = param([2.2, 1.0, 2.0, 0.4])
params = Flux.Params([p])
function predict_rd_dde()
  Array(diffeq_rd(p,prob,MethodOfSteps(Tsit5()),saveat=0.1))
end
loss_rd_dde() = sum(abs2,x-1 for x in predict_rd_dde())
loss_rd_dde()</code></pre><p>Or we can use a stochastic differential equation:</p><pre><code class="language-julia">function lotka_volterra_noise(du,u,p,t)
  du[1] = 0.1u[1]
  du[2] = 0.1u[2]
end
prob = SDEProblem(lotka_volterra,lotka_volterra_noise,[1.0,1.0],(0.0,10.0))

p = param([2.2, 1.0, 2.0, 0.4])
params = Flux.Params([p])
function predict_fd_sde()
  diffeq_fd(p,reduction,101,prob,SOSRI(),saveat=0.1)
end
loss_fd_sde() = sum(abs2,x-1 for x in predict_fd_sde())
loss_fd_sde()

data = Iterators.repeated((), 100)
opt = ADAM(0.1)
cb = function ()
  display(loss_fd_sde())
  display(plot(solve(remake(prob,p=Flux.data(p)),SOSRI(),saveat=0.1),ylim=(0,6)))
end

# Display the ODE with the current parameter values.
cb()

Flux.train!(loss_fd_sde, params, data, opt, cb = cb)</code></pre><p><img src="https://user-images.githubusercontent.com/1814174/51399524-2c6abf80-1b14-11e9-96ae-0192f7debd03.gif" alt="SDE NN Animation"/></p><h3><a class="nav-anchor" id="Neural-Ordinary-Differential-Equations-1" href="#Neural-Ordinary-Differential-Equations-1">Neural Ordinary Differential Equations</a></h3><p>We can use DiffEqFlux.jl to define, solve, and train neural ordinary differential equations. A neural ODE is an ODE where a neural network defines its derivative function. Thus for example, with the multilayer perceptron neural network <code>Chain(Dense(2,50,tanh),Dense(50,2))</code>, a neural ODE would be defined as having the ODE function:</p><pre><code class="language-julia">model = Chain(Dense(2,50,tanh),Dense(50,2))
# Define the ODE as the forward pass of the neural network with weights `p`
dudt_(du,u,p,t) = du .= model(u)</code></pre><p>A convenience function which handles all of the details is <code>neural_ode</code>. To use <code>neural_ode</code>, you give it the initial condition, the internal neural network model to use, the timespan to solve on, and any ODE solver arguments. For example, this neural ODE would be defined as:</p><pre><code class="language-julia">tspan = (0.0f0,25.0f0)
x-&gt;neural_ode(x,dudt,tspan,Tsit5(),saveat=0.1)</code></pre><p>where here we made it a layer that takes in the initial condition and spits out an array for the time series saved at every 0.1 time steps.</p><h3><a class="nav-anchor" id="Training-a-Neural-Ordinary-Differential-Equation-1" href="#Training-a-Neural-Ordinary-Differential-Equation-1">Training a Neural Ordinary Differential Equation</a></h3><p>Let&#39;s get a time series array from the Lotka-Volterra equation as data:</p><pre><code class="language-julia">function lotka_volterra(du,u,p,t)
  x, y = u
  α, β, δ, γ = p
  du[1] = dx = α*x - β*x*y
  du[2] = dy = -δ*y + γ*x*y
end
u0 = [1.0,1.0]
tspan = (0.0,10.0)
p = [1.5,1.0,3.0,1.0]
prob = ODEProblem(lotka_volterra,u0,tspan,p)
ode_data = Array(solve(prob,Tsit5(),saveat=0.1))</code></pre><p>Now let&#39;s define a neural network with a <code>neural_ode</code> layer. First we define the layer:</p><pre><code class="language-julia">dudt = Chain(Dense(2,50,tanh),Dense(50,2))
tspan = (0.0f0,10.0f0)
n_ode = x-&gt;neural_ode(x,dudt,tspan,Tsit5(),saveat=0.1)</code></pre><p>And build a neural network around it. We will use the L2 loss of the network&#39;s output against the time series data:</p><pre><code class="language-julia">function predict_n_ode()
  n_ode(u0)
end
loss_n_ode() = sum(abs2,ode_data .- predict_n_ode())</code></pre><p>and then train the neural network to learn the ODE:</p><pre><code class="language-julia">data = Iterators.repeated((), 100)
opt = ADAM(0.1)
cb = function () #callback function to observe training
  display(loss_n_ode())
  # plot current prediction against data
  cur_pred = predict_n_ode()
  pl = scatter(0.0:0.1:10.0,ode_data[1,:],label=&quot;data&quot;)
  scatter!(pl,0.0:0.1:10.0,cur_pred[1,:],label=&quot;prediction&quot;)
  plot(pl)
end

# Display the ODE with the initial parameter values.
cb()

Flux.train!(loss_n_ode, params, data, opt, cb = cb)</code></pre><h2><a class="nav-anchor" id="Use-with-GPUs-1" href="#Use-with-GPUs-1">Use with GPUs</a></h2><p>Note that the differential equation solvers will run on the GPU if the initial condition is a GPU array. Thus for example, we can define a neural ODE by hand that runs on the GPU:</p><pre><code class="language-julia">u0 = [2.; 0.] |&gt; gpu
dudt = Chain(Dense(2,50,tanh),Dense(50,2)) |&gt; gpu

function ODEfunc(du,u,p,t)
    du .= Flux.data(dudt(u))
end
prob = ODEProblem(ODEfunc, u0,tspan)

# Runs on a GPU
sol = solve(prob,BS3(),saveat=0.1)</code></pre><p>and the <code>diffeq</code> layer functions can be used similarly. Or we can directly use the neural ODE layer function, like:</p><pre><code class="language-julia">x-&gt;neural_ode(gpu(x),gpu(dudt),tspan,BS3(),saveat=0.1)</code></pre><h2><a class="nav-anchor" id="API-Documentation-1" href="#API-Documentation-1">API Documentation</a></h2><h3><a class="nav-anchor" id="DiffEq-Layer-Functions-1" href="#DiffEq-Layer-Functions-1">DiffEq Layer Functions</a></h3><ul><li><code>diffeq_rd(p,prob, args...;u0 = prob.u0, kwargs...)</code> uses Flux.jl&#39;s reverse-mode AD through the differential equation solver with parameters <code>p</code> and initial condition <code>u0</code>. The rest of the arguments are passed to the differential equation solver. The return is the DESolution.</li><li><code>diffeq_fd(p,reduction,n,prob,args...;u0 = prob.u0, kwargs...)</code> uses ForwardDiff.jl&#39;s forward-mode AD through the differential equation solver with parameters <code>p</code> and initial condition <code>u0</code>. <code>n</code> is the output size where the return value is <code>reduction(sol)</code>. The rest of the arguments are passed to the differential equation solver.</li><li><code>diffeq_adjoint(p,prob,args...;u0 = prob.u0, kwargs...)</code> uses adjoint sensitivity analysis to  &quot;backprop the ODE solver&quot; via DiffEqSensitivity.jl. The return is the time series of the solution as an array solved with parameters <code>p</code> and initial condition <code>u0</code>. The rest of the arguments are passed to the differential equation solver or handled by the adjoint sensitivity algorithm (for more details on sensitivity arguments, see <a href="http://docs.juliadiffeq.org/latest/analysis/sensitivity.html#Adjoint-Sensitivity-Analysis-1">the diffeq documentation</a>)</li></ul><h3><a class="nav-anchor" id="Neural-DE-Layer-Functions-1" href="#Neural-DE-Layer-Functions-1">Neural DE Layer Functions</a></h3><ul><li><code>neural_ode(x,model,tspan,args...;kwargs...)</code> defines a neural ODE layer where <code>x</code> is the initial condition, <code>model</code> is a Flux.jl model, <code>tspan</code> is the time span to integrate, and the rest of the arguments are passed to the ODE solver. The parameters should be implicit in the <code>model</code>.</li><li><code>nueral_msde(x,model,mp,tspan,args...;kwargs)</code> defines a neural multiplicative SDE layer where <code>x</code> is the initial condition, <code>model</code> is a Flux.jl model, <code>tspan</code> is the time span to integrate, and the rest of the arguments are passed to the SDE solver. The noise is assumed to be diagonal multiplicative, i.e. the Wiener term is <code>mp.*u.*dW</code> for some array of noise constants <code>mp</code>.</li></ul><footer><hr/><a class="next" href="autodocs/"><span class="direction">Next</span><span class="title">Docstrings</span></a></footer></article></body></html>
